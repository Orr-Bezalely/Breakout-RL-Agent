{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9f02e52",
   "metadata": {},
   "source": [
    "# CM30359: Investigation of Deep Q-learning on Breakout - Group 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0c9e8",
   "metadata": {},
   "source": [
    "## 1. Imports and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceedc2d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ceedc2d6",
    "outputId": "c9742f9b-55ff-437a-aed7-15d7bb52408e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "!pip install gym[classic_control,atari,accept-rom-license]==0.26.0\n",
    "!pip install typing-extensions --upgrade\n",
    "!pip install moviepy\n",
    "\n",
    "import gym\n",
    "from gym.utils.save_video import save_video\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count, compress\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from gym.wrappers import AtariPreprocessing, FrameStack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb1c0df",
   "metadata": {},
   "source": [
    "- ### GPU utilisation check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb89a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "98eb89a6",
    "outputId": "201bfce3-e8b2-4112-be24-9954ff595e90"
   },
   "outputs": [],
   "source": [
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "plt.ion()\n",
    "\n",
    "# Check if GPU can be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e201182",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac155b",
   "metadata": {
    "id": "98ac155b"
   },
   "outputs": [],
   "source": [
    "def Preprocessing_env(env):\n",
    "\n",
    "    env = gym.wrappers.AtariPreprocessing(env, noop_max=30, \n",
    "                                      screen_size=84, terminal_on_life_loss=False, \n",
    "                                      grayscale_obs=True, grayscale_newaxis=False, scale_obs=False)\n",
    "\n",
    "    env = gym.wrappers.FrameStack(env, 4)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e77c4",
   "metadata": {},
   "source": [
    "## 3. Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c21b64",
   "metadata": {},
   "source": [
    "- ### Dueling DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae9a0ca",
   "metadata": {
    "id": "6ae9a0ca"
   },
   "outputs": [],
   "source": [
    "class DDDQNModel(nn.Module): # DDDQN\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DDDQNModel, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        conv_out_size = self.get_conv_out_size(input_shape)\n",
    "\n",
    "        self.state_value = nn.Sequential(\n",
    "                    nn.Linear(conv_out_size, 512),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(512, 1)\n",
    "                )\n",
    "        self.action_advantage = nn.Sequential(\n",
    "                nn.Linear(conv_out_size, 512),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(512, n_actions)\n",
    "            )\n",
    "        \n",
    "    def get_conv_out_size(self, shape):\n",
    "        conv_size = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(conv_size.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        advantage = self.action_advantage(conv_out)      \n",
    "        return self.state_value(conv_out) + torch.sub(advantage, torch.mean(advantage))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a34a72",
   "metadata": {},
   "source": [
    "- ### DQN/DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a86688",
   "metadata": {
    "id": "13a86688"
   },
   "outputs": [],
   "source": [
    "class DQNModel(nn.Module): # DQN/DDQN\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQNModel, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        conv_out_size = self.get_conv_out_size(input_shape)\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def get_conv_out_size(self, shape):\n",
    "        conv_size = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(conv_size.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)   \n",
    "        return self.value(conv_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821fd96f",
   "metadata": {},
   "source": [
    "- ## Memory Efficiency Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1183f4c",
   "metadata": {
    "id": "f1183f4c"
   },
   "outputs": [],
   "source": [
    "## Attempt at memory efficient\n",
    "\n",
    "# Experience = collections.namedtuple('Experience', field_names=['state', 'action', 'reward', \n",
    "#            'done', 'new_state'])\n",
    "# Samplable = collections.namedtuple('Samplable', field_names=['samplable'])\n",
    "\n",
    "# class ExperienceReplay:\n",
    "#     def __init__(self, capacity):\n",
    "#         self.buffer = collections.deque(maxlen=capacity)\n",
    "#         self.sample_buffer = collections.deque(maxlen=capacity)\n",
    "#     def __len__(self):\n",
    "#         return len(self.buffer)\n",
    "#     def append(self, state,action,reward,is_done,new_state,first_samplable):\n",
    "#         if first_samplable == 0: # AKA if this is 4th frame of the game\n",
    "# #             print(\"First frame\")\n",
    "# #             print(np.shape(state))\n",
    "# #             print(np.shape(state[0]))\n",
    "# #             self.buffer.append(Experience(state[i],action,reward,is_done,new_state[i]) for i in range(3))\n",
    "#             self.buffer.append(Experience(state[0],action,reward,is_done,new_state[0]))\n",
    "#             self.buffer.append(Experience(state[1],action,reward,is_done,new_state[1]))\n",
    "#             self.buffer.append(Experience(state[2],action,reward,is_done,new_state[2]))\n",
    "#             self.sample_buffer.append(Samplable(False))\n",
    "#             self.sample_buffer.append(Samplable(False))\n",
    "#             self.sample_buffer.append(Samplable(False))\n",
    "# #         print(np.shape(self.buffer[0][0]))\n",
    "\n",
    "#         self.buffer.append(Experience(state[3],action,reward,is_done,new_state[3]))\n",
    "#         self.sample_buffer.append(Samplable(True))\n",
    "#         for i in range(3): \n",
    "#             self.sample_buffer[i] = False\n",
    "#         #print(sys.getsizeof(self.buffer[0][0]))\n",
    "  \n",
    "#     def batch_sample(self, batch_size):\n",
    "#         samplable_list = list(compress(range(len(self.sample_buffer)), self.sample_buffer))\n",
    "#         indices = np.random.choice(len(samplable_list), batch_size, replace=False)\n",
    "#         states, actions, rewards, dones, new_states = [], [], [], [], []\n",
    "#         for idx in indices:\n",
    "#             states.append(np.stack([self.buffer[idx-i][0] for i in range(4)], 0))\n",
    "#             actions.append(self.buffer[idx][1])\n",
    "#             rewards.append(self.buffer[idx][2])\n",
    "#             dones.append(self.buffer[idx][3])\n",
    "#             new_states.append(np.stack([self.buffer[idx-i][4] for i in range(4)], 0))\n",
    "            \n",
    "# #         print(np.shape(states))\n",
    "# #         print(np.shape(states[0]))\n",
    "        \n",
    "# #         print(np.shape(actions))\n",
    "# #         print(np.shape(actions[0]))\n",
    "              \n",
    "#         return np.array(states), np.array(actions), np.array(rewards,dtype=np.float32), np.array(dones, dtype=np.uint8),np.array(new_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2af7b4",
   "metadata": {},
   "source": [
    "## 4. Experience Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b1ced1",
   "metadata": {
    "id": "31b1ced1"
   },
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', field_names=['state', 'action', 'reward', \n",
    "           'done', 'next_state'])\n",
    "\n",
    "class ExperienceReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def append(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "  \n",
    "    def batch_sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size,\n",
    "                replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*[self.buffer[idx] for idx in indices])\n",
    "        return np.array(states), np.array(actions), np.array(rewards, dtype=np.float32), np.array(dones, dtype=np.uint8),np.array(next_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80961d22",
   "metadata": {},
   "source": [
    "## 5. Graph results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdea47c",
   "metadata": {
    "id": "dbdea47c"
   },
   "outputs": [],
   "source": [
    "class Graphing:\n",
    "    def __init__(self):\n",
    "        self.writer = SummaryWriter(comment=\"-\" + \"ALE/BreakoutNoFrameskip-v4\")\n",
    "        self.episode_rewards = []\n",
    "        self.best_mean_reward = float(\"-inf\")\n",
    "    def add_info(self, reward, frame_idx, epsilon):\n",
    "        self.episode_rewards.append(reward)\n",
    "        mean_reward = self.get_mean_reward()\n",
    "        self.writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
    "        self.writer.add_scalar(\"reward_100\", mean_reward, frame_idx)\n",
    "        self.writer.add_scalar(\"reward\", reward, frame_idx)\n",
    "        if self.best_mean_reward < mean_reward:\n",
    "            self.best_mean_reward = mean_reward\n",
    "            return True\n",
    "        return False\n",
    "    def get_best_reward(self):\n",
    "        return self.best_mean_reward\n",
    "    def get_mean_reward(self):\n",
    "        return np.mean(self.episode_rewards[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f674f4c",
   "metadata": {},
   "source": [
    "## 6. Process Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246de535",
   "metadata": {
    "id": "246de535"
   },
   "outputs": [],
   "source": [
    "class ProcessingEnv: \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.episode = 0\n",
    "        self.video = []\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.episode += 1\n",
    "        save_video(self.video, \"videos\", fps=25,\n",
    "        episode_trigger = lambda x: x in [10] or x % 250 == 0,\n",
    "        episode_index=self.episode)\n",
    "        self.video = []\n",
    "        self.state, _ = self.env.reset()\n",
    "        self.fire()\n",
    "        self.total_rewards = 0.0\n",
    "\n",
    "    def get_episode(self):\n",
    "        return self.episode\n",
    "\n",
    "    def get_total_rewards(self):\n",
    "        return self.total_rewards\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.state\n",
    "        \n",
    "    def set_state(self, state):\n",
    "        self.state = state\n",
    "\n",
    "    def fire(self):\n",
    "        self.state, _, _, _, _ = self.env.step(1)\n",
    "\n",
    "    def step(self, action):\n",
    "        next_state, reward, is_done, truncated, info = self.env.step(action)\n",
    "        self.total_rewards += reward\n",
    "        self.video.append(self.env.render())\n",
    "        \n",
    "        return next_state, reward, is_done, truncated, info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f690fb77",
   "metadata": {},
   "source": [
    "- ## Start tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bc7277",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "53bc7277",
    "outputId": "075e4117-a76f-443c-fcf2-3bd249bce021"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "import datetime\n",
    "print(\">>>Training starts at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf245a",
   "metadata": {},
   "source": [
    "## 7. Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6705c315",
   "metadata": {},
   "source": [
    "- ### DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JB08DmiZZBKo",
   "metadata": {
    "id": "JB08DmiZZBKo"
   },
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, env, gamma, batch_size, experience_replay_buffer_size, learning_rate, \n",
    "                 target_network_update_frequency, experience_replay_start_size, update_frequency,\n",
    "                 epsilon_start, epsilon_decay, epsilon_end):\n",
    "        self.env = env\n",
    "        self.GAMMA = gamma                   \n",
    "        self.BATCH_SIZE = batch_size                \n",
    "        self.EXPERIENCE_REPLAY_BUFFER_SIZE = experience_replay_buffer_size           \n",
    "        self.LEARNING_RATE = learning_rate           \n",
    "        self.TARGET_NETWORK_UPDATE_FREQUENCY = target_network_update_frequency      \n",
    "        self.EXPERIENCE_REPLAY_START_SIZE = experience_replay_start_size     \n",
    "        self.UPDATE_FREQUENCY = update_frequency\n",
    "\n",
    "        self.EPSILON_START = epsilon_start\n",
    "        self.EPSILON_DECAY = epsilon_decay\n",
    "        self.EPSILON_END = epsilon_end\n",
    "        self.buffer = ExperienceReplayBuffer(self.EXPERIENCE_REPLAY_BUFFER_SIZE)\n",
    "        self.net = DQNModel(env.env.observation_space.shape, env.env.action_space.n).to(device)\n",
    "        self.target_net = DQNModel(env.env.observation_space.shape, env.env.action_space.n).to(device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.LEARNING_RATE)\n",
    "        self.criterion_loss = nn.SmoothL1Loss()\n",
    "\n",
    "        self.frame_idx = 1\n",
    "        self.num_of_lives = 5\n",
    "\n",
    "\n",
    "\n",
    "        restart = False\n",
    "        if(restart == True):\n",
    "            checkpoint1 = torch.load('checkpoint1Score109.pth')\n",
    "            print(\"Loaded checkpoint 1\")\n",
    "            checkpoint2 = torch.load('checkpoint2Score109.pth')\n",
    "            print(\"Loaded checkpoint 2\")\n",
    "            self.net.load_state_dict(checkpoint1['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint1['optimizer_state_dict'])\n",
    "            self.EPSILON_START = checkpoint1['epsilon']\n",
    "            print(self.EPSILON_START)\n",
    "            self.criterion_loss = checkpoint1['loss']\n",
    "            self.target_net.load_state_dict(checkpoint2['model_state_dict'])\n",
    "\n",
    "\n",
    "        self.net.train()\n",
    "        self.target_net.train()\n",
    "        self.epsilon = self.EPSILON_START\n",
    "        self.graph = Graphing()\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.env.env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([state], copy=False)\n",
    "            state_v = torch.tensor(state_a, dtype=torch.float32).to(device)\n",
    "            q_vals_v = self.net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "        return action\n",
    "\n",
    "    def train(self, episode_num):\n",
    "        while True:\n",
    "            state = self.env.get_state()\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, is_done, truncated, info = self.env.step(action)\n",
    "\n",
    "            # CROP REWARDS - IMPORTANT\n",
    "            reward = min(reward, 1)\n",
    "\n",
    "            # STORING IN EXPERIENCE REPLAY BUFFER\n",
    "            if(info.get(\"lives\") < self.num_of_lives):\n",
    "                # LIFE LOSS CONSIDERED TERMINAL\n",
    "                experience = Experience(state,action,reward,True,next_state)\n",
    "                self.env.fire()\n",
    "                self.num_of_lives = info.get(\"lives\")\n",
    "            else:\n",
    "                experience = Experience(state,action,reward,is_done,next_state)\n",
    "                self.env.set_state(next_state)\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "\n",
    "            if is_done:\n",
    "                self.num_of_lives = 5\n",
    "                best_so_far = self.graph.add_info(self.env.get_total_rewards(), self.frame_idx, self.epsilon)\n",
    "                if best_so_far:\n",
    "                    torch.save({'model_state_dict': self.net.state_dict(),'optimizer_state_dict': self.optimizer.state_dict(),'epsilon': self.epsilon,'loss': self.criterion_loss},'checkpoint1.pth')\n",
    "                    print(\"Best mean reward updated %.3f, %d\" % (self.graph.get_best_reward(), self.frame_idx))\n",
    "                self.env.reset()\n",
    "\n",
    "\n",
    "            if self.frame_idx >= 4000000 or self.env.episode >= episode_num:\n",
    "                print(\"Solved in %d frames!\" % self.frame_idx)\n",
    "                torch.save({'model_state_dict': self.net.state_dict()}, 'checkpoint3.pth')\n",
    "                break\n",
    "\n",
    "            if len(self.buffer) < self.EXPERIENCE_REPLAY_START_SIZE:\n",
    "                self.frame_idx += 1\n",
    "                continue\n",
    "            \n",
    "            self.epsilon = np.interp(self.frame_idx, [0, self.EPSILON_DECAY], [self.EPSILON_START, self.EPSILON_END])\n",
    "\n",
    "            if self.frame_idx % self.UPDATE_FREQUENCY == 0:\n",
    "                batch = self.buffer.batch_sample(self.BATCH_SIZE)\n",
    "                states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "                states_v = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "                next_states_v = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "                actions_v = torch.tensor(actions, dtype=torch.int64).to(device)\n",
    "                rewards_v = torch.tensor(rewards).to(device)\n",
    "                done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "                q_vals = self.net(states_v)\n",
    "                state_action_values = q_vals.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    next_state_values = self.target_net(next_states_v).max(1)[0]  # DQN\n",
    "\n",
    "                next_state_values[done_mask] = 0.0\n",
    "                next_state_values = next_state_values.detach()\n",
    "\n",
    "                estimated_state_action_values = rewards_v + self.GAMMA * next_state_values \n",
    "\n",
    "                loss_huber = self.criterion_loss(state_action_values, estimated_state_action_values)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss_huber.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if self.frame_idx % self.TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "                print(\"%d:  %d games, mean reward %.3f, (epsilon %.2f)\" % (self.frame_idx, self.env.get_episode(), self.graph.get_mean_reward(), self.epsilon))\n",
    "                self.target_net.load_state_dict(self.net.state_dict())\n",
    "                torch.save({'model_state_dict': self.target_net.state_dict()}, 'checkpoint2.pth')\n",
    "            self.frame_idx += 1\n",
    "            \n",
    "            \n",
    "    def play(self, episode_num):\n",
    "        checkpoint1 = torch.load('checkpoint2DQN.pth')\n",
    "        print(\"Loaded checkpoint 1\")\n",
    "        self.net.load_state_dict(checkpoint1['model_state_dict'])\n",
    "        self.epsilon = 0.001\n",
    "        episode_rewards = []\n",
    "        for episode in range(episode_num):\n",
    "            self.frame_idx = 0\n",
    "            self.num_of_lives = 5\n",
    "            self.env.reset()\n",
    "            is_done = False\n",
    "            while not is_done:\n",
    "                state = self.env.get_state()\n",
    "                action = self.get_action(state)\n",
    "                next_state, _, is_done, _, info = self.env.step(action)\n",
    "                \n",
    "                if(info.get(\"lives\") < self.num_of_lives):\n",
    "                    print(\"Lost life\")\n",
    "                # LIFE LOSS CONSIDERED TERMINAL\n",
    "                    self.env.fire()\n",
    "                    self.num_of_lives = info.get(\"lives\")\n",
    "                else:\n",
    "                    self.env.set_state(next_state)\n",
    "                    \n",
    "                if(self.frame_idx >= 10000):\n",
    "                    break\n",
    "                self.frame_idx += 1\n",
    "                \n",
    "            total_reward = self.env.get_total_rewards()\n",
    "            print(total_reward)\n",
    "            print(self.frame_idx)\n",
    "            episode_rewards.append(total_reward)\n",
    "        return episode_rewards, max(episode_rewards), sum(episode_rewards) / len(episode_rewards)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "processed_env = ProcessingEnv(Preprocessing_env(env))\n",
    "\n",
    "\n",
    "dqn = DQN(env=processed_env, gamma=0.99, batch_size=32, experience_replay_buffer_size=100000, \n",
    "              learning_rate=1e-4, target_network_update_frequency=10000, experience_replay_start_size=5000, \n",
    "              update_frequency=4, epsilon_start=1, epsilon_decay=500000, epsilon_end=0.01)\n",
    "\n",
    "# print(dqn.play(30))\n",
    "dqn.train(5550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91c6132",
   "metadata": {},
   "source": [
    "- ### DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2j-XP43AZBS_",
   "metadata": {
    "id": "2j-XP43AZBS_"
   },
   "outputs": [],
   "source": [
    "class DDQN:\n",
    "    def __init__(self, env, gamma, batch_size, experience_replay_buffer_size, learning_rate, \n",
    "                 target_network_update_frequency, experience_replay_start_size, update_frequency,\n",
    "                 epsilon_start, epsilon_decay, epsilon_end):\n",
    "        self.env = env\n",
    "        self.GAMMA = gamma                   \n",
    "        self.BATCH_SIZE = batch_size                \n",
    "        self.EXPERIENCE_REPLAY_BUFFER_SIZE = experience_replay_buffer_size           \n",
    "        self.LEARNING_RATE = learning_rate           \n",
    "        self.TARGET_NETWORK_UPDATE_FREQUENCY = target_network_update_frequency      \n",
    "        self.EXPERIENCE_REPLAY_START_SIZE = experience_replay_start_size     \n",
    "        self.UPDATE_FREQUENCY = update_frequency\n",
    "\n",
    "        self.EPSILON_START = epsilon_start\n",
    "        self.EPSILON_DECAY = epsilon_decay\n",
    "        self.EPSILON_END = epsilon_end\n",
    "        self.buffer = ExperienceReplayBuffer(self.EXPERIENCE_REPLAY_BUFFER_SIZE)\n",
    "        self.net = DQNModel(env.env.observation_space.shape, env.env.action_space.n).to(device)\n",
    "        self.target_net = DQNModel(env.env.observation_space.shape, env.env.action_space.n).to(device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.LEARNING_RATE)\n",
    "        self.criterion_loss = nn.SmoothL1Loss()\n",
    "\n",
    "        self.frame_idx = 1\n",
    "        self.num_of_lives = 5\n",
    "\n",
    "\n",
    "\n",
    "        restart = False\n",
    "        if(restart == True):\n",
    "            checkpoint1 = torch.load('checkpoint1Score109.pth')\n",
    "            print(\"Loaded checkpoint 1\")\n",
    "            checkpoint2 = torch.load('checkpoint2Score109.pth')\n",
    "            print(\"Loaded checkpoint 2\")\n",
    "            self.net.load_state_dict(checkpoint1['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint1['optimizer_state_dict'])\n",
    "            self.EPSILON_START = checkpoint1['epsilon']\n",
    "            print(self.EPSILON_START)\n",
    "            self.criterion_loss = checkpoint1['loss']\n",
    "            self.target_net.load_state_dict(checkpoint2['model_state_dict'])\n",
    "\n",
    "\n",
    "        self.net.train()\n",
    "        self.target_net.train()\n",
    "        self.epsilon = self.EPSILON_START\n",
    "        self.graph = Graphing()\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.env.env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([state], copy=False)\n",
    "            state_v = torch.tensor(state_a, dtype=torch.float32).to(device)\n",
    "            q_vals_v = self.net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "        return action\n",
    "\n",
    "    def train(self, episode_num):\n",
    "        while True:\n",
    "            state = self.env.get_state()\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, is_done, truncated, info = self.env.step(action)\n",
    "\n",
    "            # CROP REWARDS - IMPORTANT\n",
    "#             reward = min(reward, 1)\n",
    "\n",
    "            # STORING IN EXPERIENCE REPLAY BUFFER\n",
    "            if(info.get(\"lives\") < self.num_of_lives):\n",
    "                # LIFE LOSS CONSIDERED TERMINAL\n",
    "                experience = Experience(state,action,reward,True,next_state)\n",
    "                self.env.fire()\n",
    "                self.num_of_lives = info.get(\"lives\")\n",
    "            else:\n",
    "                experience = Experience(state,action,reward,is_done,next_state)\n",
    "                self.env.set_state(next_state)\n",
    "#             experience = Experience(state,action,reward,is_done,next_state)\n",
    "#             self.env.set_state(next_state)\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "            if is_done:\n",
    "                self.num_of_lives = 5\n",
    "                best_so_far = self.graph.add_info(self.env.get_total_rewards(), self.frame_idx, self.epsilon)\n",
    "                if best_so_far:\n",
    "                    torch.save({'model_state_dict': self.net.state_dict(),'optimizer_state_dict': self.optimizer.state_dict(),'epsilon': self.epsilon,'loss': self.criterion_loss},'checkpoint1.pth')\n",
    "                    print(\"Best mean reward updated %.3f, %d\" % (self.graph.get_best_reward(), self.frame_idx))\n",
    "                self.env.reset()\n",
    "\n",
    "\n",
    "            if self.frame_idx >= 4000000 or self.env.episode >= episode_num:\n",
    "                print(\"Solved in %d frames!\" % self.frame_idx)\n",
    "                torch.save({'model_state_dict': self.net.state_dict()}, 'checkpoint3.pth')\n",
    "                break\n",
    "\n",
    "            if len(self.buffer) < self.EXPERIENCE_REPLAY_START_SIZE:\n",
    "                self.frame_idx += 1\n",
    "                continue\n",
    "            \n",
    "            self.epsilon = np.interp(self.frame_idx, [0, self.EPSILON_DECAY], [self.EPSILON_START, self.EPSILON_END])\n",
    "\n",
    "            if self.frame_idx % self.UPDATE_FREQUENCY == 0:\n",
    "                batch = self.buffer.batch_sample(self.BATCH_SIZE)\n",
    "                states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "                states_v = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "                next_states_v = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "                actions_v = torch.tensor(actions, dtype=torch.int64).to(device)\n",
    "                rewards_v = torch.tensor(rewards).to(device)\n",
    "                done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "                q_vals = self.net(states_v)\n",
    "                state_action_values = q_vals.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                q_vals2 = self.net(next_states_v)\n",
    "                _, act_v = torch.max(q_vals2, dim=1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    next_state_values1 = self.target_net(next_states_v)\n",
    "                    next_state_values2 = next_state_values1.gather(1, act_v.unsqueeze(1))\n",
    "                    next_state_values = torch.reshape(next_state_values2, (-1,))\n",
    "\n",
    "                next_state_values[done_mask] = 0.0\n",
    "                next_state_values = next_state_values.detach()\n",
    "\n",
    "                estimated_state_action_values = rewards_v + self.GAMMA * next_state_values \n",
    "\n",
    "                loss_huber = self.criterion_loss(state_action_values, estimated_state_action_values)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss_huber.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if self.frame_idx % self.TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "                print(\"%d:  %d games, mean reward %.3f, (epsilon %.2f)\" % (self.frame_idx, self.env.get_episode(), self.graph.get_mean_reward(), self.epsilon))\n",
    "                self.target_net.load_state_dict(self.net.state_dict())\n",
    "                torch.save({'model_state_dict': self.target_net.state_dict()}, 'checkpoint2.pth')\n",
    "            self.frame_idx += 1\n",
    "\n",
    "\n",
    "    def play(self, episode_num):\n",
    "        checkpoint1 = torch.load('checkpoint2DDQN20k2.pth')\n",
    "        print(\"Loaded checkpoint 1\")\n",
    "        self.net.load_state_dict(checkpoint1['model_state_dict'])\n",
    "        self.epsilon = 0.05\n",
    "        episode_rewards = []\n",
    "        for episode in range(episode_num):\n",
    "            self.frame_idx = 0\n",
    "            self.num_of_lives = 5\n",
    "            self.env.reset()\n",
    "            is_done = False\n",
    "            while not is_done:\n",
    "                state = self.env.get_state()\n",
    "                action = self.get_action(state)\n",
    "                next_state, _, is_done, _, info = self.env.step(action)\n",
    "                \n",
    "                if(info.get(\"lives\") < self.num_of_lives):\n",
    "                    print(\"Lost life\")\n",
    "                # LIFE LOSS CONSIDERED TERMINAL\n",
    "                    self.env.fire()\n",
    "                    self.num_of_lives = info.get(\"lives\")\n",
    "                else:\n",
    "                    self.env.set_state(next_state)\n",
    "                    \n",
    "                if(self.frame_idx >= 18000):\n",
    "                    break\n",
    "                self.frame_idx += 1\n",
    "                \n",
    "            total_reward = self.env.get_total_rewards()\n",
    "            print(total_reward)\n",
    "            print(self.frame_idx)\n",
    "            episode_rewards.append(total_reward)\n",
    "        return episode_rewards, max(episode_rewards), sum(episode_rewards) / len(episode_rewards)\n",
    "            \n",
    "\n",
    "\n",
    "env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "processed_env = ProcessingEnv(Preprocessing_env(env))\n",
    "\n",
    "\n",
    "ddqn = DDQN(env=processed_env, gamma=0.99, batch_size=32, experience_replay_buffer_size=100000, \n",
    "              learning_rate=1e-4, target_network_update_frequency=10000, experience_replay_start_size=5000, \n",
    "              update_frequency=4, epsilon_start=1, epsilon_decay=500000, epsilon_end=0.01)\n",
    "ddqn.train(5550)\n",
    "# print(ddqn.play(30))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af57734",
   "metadata": {},
   "source": [
    "- ### DDDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d58e63",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 956
    },
    "id": "01d58e63",
    "outputId": "0de05872-059e-49ce-c084-749e53efa1de"
   },
   "outputs": [],
   "source": [
    "class DDDQN:\n",
    "    def __init__(self, env, gamma, batch_size, experience_replay_buffer_size, learning_rate, \n",
    "                 target_network_update_frequency, experience_replay_start_size, update_frequency,\n",
    "                 epsilon_start, epsilon_decay, epsilon_end):\n",
    "        self.env = env\n",
    "        self.GAMMA = gamma                   \n",
    "        self.BATCH_SIZE = batch_size                \n",
    "        self.EXPERIENCE_REPLAY_BUFFER_SIZE = experience_replay_buffer_size           \n",
    "        self.LEARNING_RATE = learning_rate           \n",
    "        self.TARGET_NETWORK_UPDATE_FREQUENCY = target_network_update_frequency      \n",
    "        self.EXPERIENCE_REPLAY_START_SIZE = experience_replay_start_size     \n",
    "        self.UPDATE_FREQUENCY = update_frequency\n",
    "\n",
    "        self.EPSILON_START = epsilon_start\n",
    "        self.EPSILON_DECAY = epsilon_decay\n",
    "        self.EPSILON_END = epsilon_end\n",
    "        self.buffer = ExperienceReplayBuffer(self.EXPERIENCE_REPLAY_BUFFER_SIZE)\n",
    "        self.net = DDDQNModel(env.env.observation_space.shape, env.env.action_space.n).to(device)\n",
    "        self.target_net = DDDQNModel(env.env.observation_space.shape, env.env.action_space.n).to(device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=self.LEARNING_RATE)\n",
    "        self.criterion_loss = nn.SmoothL1Loss()\n",
    "\n",
    "        self.frame_idx = 1\n",
    "        self.num_of_lives = 5\n",
    "\n",
    "\n",
    "\n",
    "        restart = False\n",
    "        if(restart == True):\n",
    "            checkpoint1 = torch.load('checkpoint1Score109.pth')\n",
    "            print(\"Loaded checkpoint 1\")\n",
    "            checkpoint2 = torch.load('checkpoint2Score109.pth')\n",
    "            print(\"Loaded checkpoint 2\")\n",
    "            self.net.load_state_dict(checkpoint1['model_state_dict'])\n",
    "            self.optimizer.load_state_dict(checkpoint1['optimizer_state_dict'])\n",
    "            self.EPSILON_START = checkpoint1['epsilon']\n",
    "            print(self.EPSILON_START)\n",
    "            self.criterion_loss = checkpoint1['loss']\n",
    "            self.target_net.load_state_dict(checkpoint2['model_state_dict'])\n",
    "\n",
    "\n",
    "        self.net.train()\n",
    "        self.target_net.train()\n",
    "        self.epsilon = self.EPSILON_START\n",
    "        self.graph = Graphing()\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = self.env.env.action_space.sample()\n",
    "        else:\n",
    "            state_a = np.array([state], copy=False)\n",
    "            state_v = torch.tensor(state_a, dtype=torch.float32).to(device)\n",
    "            q_vals_v = self.net(state_v)\n",
    "            _, act_v = torch.max(q_vals_v, dim=1)\n",
    "            action = int(act_v.item())\n",
    "        return action\n",
    "\n",
    "    def train(self, episode_num):\n",
    "        while True:\n",
    "            state = self.env.get_state()\n",
    "            action = self.get_action(state)\n",
    "            next_state, reward, is_done, truncated, info = self.env.step(action)\n",
    "\n",
    "            # CROP REWARDS - IMPORTANT\n",
    "            reward = min(reward, 1)\n",
    "\n",
    "            # STORING IN EXPERIENCE REPLAY BUFFER\n",
    "            if(info.get(\"lives\") < self.num_of_lives):\n",
    "                # LIFE LOSS CONSIDERED TERMINAL\n",
    "                experience = Experience(state,action,reward,True,next_state)\n",
    "                self.env.fire()\n",
    "                self.num_of_lives = info.get(\"lives\")\n",
    "            else:\n",
    "                experience = Experience(state,action,reward,is_done,next_state)\n",
    "                self.env.set_state(next_state)\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "\n",
    "            if is_done:\n",
    "                self.num_of_lives = 5\n",
    "                best_so_far = self.graph.add_info(self.env.get_total_rewards(), self.frame_idx, self.epsilon)\n",
    "                if best_so_far:\n",
    "                    torch.save({'model_state_dict': self.net.state_dict(),'optimizer_state_dict': self.optimizer.state_dict(),'epsilon': self.epsilon,'loss': self.criterion_loss},'checkpoint1.pth')\n",
    "                    print(\"Best mean reward updated %.3f, %d\" % (self.graph.get_best_reward(), self.frame_idx))\n",
    "                self.env.reset()\n",
    "\n",
    "\n",
    "            if self.frame_idx >= 4000000 or self.env.episode >= episode_num:\n",
    "                print(\"Solved in %d frames!\" % self.frame_idx)\n",
    "                torch.save({'model_state_dict': self.net.state_dict()}, 'checkpoint3.pth')\n",
    "                break\n",
    "\n",
    "            if len(self.buffer) < self.EXPERIENCE_REPLAY_START_SIZE:\n",
    "                self.frame_idx += 1\n",
    "                continue\n",
    "            \n",
    "            self.epsilon = np.interp(self.frame_idx, [0, self.EPSILON_DECAY], [self.EPSILON_START, self.EPSILON_END])\n",
    "\n",
    "            if self.frame_idx % self.UPDATE_FREQUENCY == 0:\n",
    "                batch = self.buffer.batch_sample(self.BATCH_SIZE)\n",
    "                states, actions, rewards, dones, next_states = batch\n",
    "\n",
    "                states_v = torch.tensor(states, dtype=torch.float32).to(device)\n",
    "                next_states_v = torch.tensor(next_states, dtype=torch.float32).to(device)\n",
    "                actions_v = torch.tensor(actions, dtype=torch.int64).to(device)\n",
    "                rewards_v = torch.tensor(rewards).to(device)\n",
    "                done_mask = torch.ByteTensor(dones).to(device)\n",
    "\n",
    "                q_vals = self.net(states_v)\n",
    "                state_action_values = q_vals.gather(1, actions_v.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                q_vals2 = self.net(next_states_v)\n",
    "                _, act_v = torch.max(q_vals2, dim=1)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    next_state_values1 = self.target_net(next_states_v)\n",
    "                    next_state_values2 = next_state_values1.gather(1, act_v.unsqueeze(1))\n",
    "                    next_state_values = torch.reshape(next_state_values2, (-1,))\n",
    "\n",
    "                next_state_values[done_mask] = 0.0\n",
    "\n",
    "                next_state_values = next_state_values.detach()\n",
    "                \n",
    "\n",
    "                estimated_state_action_values = rewards_v + self.GAMMA * next_state_values \n",
    "\n",
    "                loss_huber = self.criterion_loss(state_action_values, estimated_state_action_values)\n",
    "                self.optimizer.zero_grad()\n",
    "                loss_huber.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if self.frame_idx % self.TARGET_NETWORK_UPDATE_FREQUENCY == 0:\n",
    "                print(\"%d:  %d games, mean reward %.3f, (epsilon %.2f)\" % (self.frame_idx, self.env.get_episode(), self.graph.get_mean_reward(), self.epsilon))\n",
    "                self.target_net.load_state_dict(self.net.state_dict())\n",
    "                torch.save({'model_state_dict': self.target_net.state_dict()}, 'checkpoint2.pth')\n",
    "                \n",
    "            self.frame_idx += 1\n",
    "            \n",
    "    def play(self, episode_num):\n",
    "        checkpoint1 = torch.load('checkpoint3DDDQN.pth')\n",
    "        print(\"Loaded checkpoint 1\")\n",
    "        self.net.load_state_dict(checkpoint1['model_state_dict'])\n",
    "        self.epsilon = 0.05\n",
    "        episode_rewards = []\n",
    "        for episode in range(episode_num):\n",
    "            self.frame_idx = 0\n",
    "            self.num_of_lives = 5\n",
    "            self.env.reset()\n",
    "            is_done = False\n",
    "            while not is_done:\n",
    "                state = self.env.get_state()\n",
    "                action = self.get_action(state)\n",
    "                next_state, _, is_done, _, info = self.env.step(action)\n",
    "                \n",
    "                if(info.get(\"lives\") < self.num_of_lives):\n",
    "                    print(\"Lost life\")\n",
    "                # LIFE LOSS CONSIDERED TERMINAL\n",
    "                    self.env.fire()\n",
    "                    self.num_of_lives = info.get(\"lives\")\n",
    "                else:\n",
    "                    self.env.set_state(next_state)\n",
    "                    \n",
    "                if(self.frame_idx >= 10000):\n",
    "                    break\n",
    "                self.frame_idx += 1\n",
    "                \n",
    "            total_reward = self.env.get_total_rewards()\n",
    "            print(total_reward)\n",
    "            print(self.frame_idx)\n",
    "            episode_rewards.append(total_reward)\n",
    "        return episode_rewards, max(episode_rewards), sum(episode_rewards) / len(episode_rewards)\n",
    "\n",
    "\n",
    "env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "processed_env = ProcessingEnv(Preprocessing_env(env))\n",
    "\n",
    "\n",
    "dddqn = DDDQN(env=processed_env, gamma=0.99, batch_size=32, experience_replay_buffer_size=100000, \n",
    "              learning_rate=1e-4, target_network_update_frequency=10000, experience_replay_start_size=5000, \n",
    "              update_frequency=4, epsilon_start=1, epsilon_decay=500000, epsilon_end=0.01)\n",
    "dddqn.train(5550)\n",
    "# print(dddqn.play(30))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec4274f",
   "metadata": {},
   "source": [
    "- ### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f2076",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RANDOM:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.frame_idx = 0\n",
    "        self.num_of_lives = 5\n",
    "        self.epsilon = 0.05\n",
    "        self.episode_rewards = []\n",
    "        self.action = 1\n",
    "\n",
    "    def play(self, episode_num):\n",
    "        for episode in range(episode_num):\n",
    "            self.frame_idx = 0\n",
    "            self.num_of_lives = 5\n",
    "            self.env.reset()\n",
    "            is_done = False\n",
    "            while not is_done:\n",
    "                state = self.env.get_state()\n",
    "                if self.frame_idx % 6 == 0:\n",
    "                    self.action = self.env.env.action_space.sample()\n",
    "                next_state, _, is_done, _, info = self.env.step(self.action)\n",
    "                if(info.get(\"lives\") < self.num_of_lives):\n",
    "                # LIFE LOSS CONSIDERED TERMINAL\n",
    "                    self.env.fire()\n",
    "                    self.num_of_lives = info.get(\"lives\")\n",
    "                else:\n",
    "                    self.env.set_state(next_state)\n",
    "                self.frame_idx += 1\n",
    "                \n",
    "            total_reward = self.env.get_total_rewards()\n",
    "            self.episode_rewards.append(total_reward)\n",
    "        return self.episode_rewards, max(self.episode_rewards), sum(self.episode_rewards) / len(self.episode_rewards)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "processed_env = ProcessingEnv(Preprocessing_env(env))\n",
    "rand = RANDOM(processed_env)\n",
    "\n",
    "print(rand.play(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e37e98",
   "metadata": {
    "id": "a9e37e98"
   },
   "outputs": [],
   "source": [
    "print(\">>>Training ends at \",datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499bfef",
   "metadata": {},
   "source": [
    " ### 8. Tensorboard Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d3e2d0",
   "metadata": {
    "id": "38d3e2d0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tensorboard  --logdir=runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77c0dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
